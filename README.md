GORANN: Recurrent and Artificial Neural Networks

## Overview

This is a from-scratch golang implementation inspired in part by the [Stanford ML course](http://cs229.stanford.edu/materials.html).

## Keywords

Gradient descent, SGD and batching, L1/L2 weight regularization, Adagrad, Adadelta, RMSprop and ADAM optimizations, hyperparameters, activation functions, input and output normalization (callbacks) 

## TODO

RNN, and more..
